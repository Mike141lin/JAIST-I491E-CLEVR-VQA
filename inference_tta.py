import os
import torch
import pandas as pd
from PIL import Image
from tqdm import tqdm
from collections import Counter
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from peft import PeftModel
from qwen_vl_utils import process_vision_info

# === å…³é”®ä¿®æ­£: æŒ‡å‘å­˜åœ¨çš„æ–‡ä»¶å¤¹ student_3b ===
ADAPTER_PATH = "./output/student_3b/final_adapter"
OUTPUT_FILE = "submission_tta.csv"
BASE_MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

DATA_ROOT = "custom_dataset"
TEST_CSV = os.path.join(DATA_ROOT, "test_non_labels.csv")
TEST_IMG_DIR = os.path.join(DATA_ROOT, "test")

print(f"ğŸš€ Running Multi-Scale TTA (1.0x, 1.2x, 0.85x) | Model: {ADAPTER_PATH}")

def run_inference():
    # åŒé‡æ£€æŸ¥è·¯å¾„
    if not os.path.exists(os.path.join(ADAPTER_PATH, "adapter_config.json")):
        print(f"âŒ ä¸¥é‡é”™è¯¯: ä¾ç„¶æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {ADAPTER_PATH}")
        return

    print("Loading model...")
    base = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID, 
        torch_dtype=torch.bfloat16, 
        attn_implementation="flash_attention_2", 
        device_map="auto"
    )
    model = PeftModel.from_pretrained(base, ADAPTER_PATH).eval()
    processor = AutoProcessor.from_pretrained(BASE_MODEL_ID)
    
    df = pd.read_csv(TEST_CSV)
    results = []
    
    print(f"Processing {len(df)} images with TTA...")
    
    for _, row in tqdm(df.iterrows(), total=len(df)):
        img_name = row['file']
        img_path = os.path.join(TEST_IMG_DIR, img_name)
        
        try: 
            raw_image = Image.open(img_path).convert("RGB")
        except:
            raw_image = Image.new('RGB', (224, 224), 'black')
            
        # === TTA: å‡†å¤‡ 3 å¼ ä¸åŒå¤§å°çš„å›¾ ===
        w, h = raw_image.size
        images_tta = [
            raw_image,                                      # 1. åŸå›¾
            raw_image.resize((int(w*1.2), int(h*1.2))),     # 2. æ”¾å¤§ (çœ‹ç»†èŠ‚)
            raw_image.resize((int(w*0.85), int(h*0.85)))    # 3. ç¼©å° (çœ‹å…¨å±€)
        ]
        
        candidates = []
        
        for img in images_tta:
            conversation = [{"role": "user", "content": [{"type": "image", "image": img}, {"type": "text", "text": f"{row['question']}\nProvide the answer and a detailed explanation."}]}]
            text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text], images=process_vision_info(conversation)[0], padding=True, return_tensors="pt").to("cuda")
            
            with torch.no_grad():
                generated_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False, temperature=0.01)
                
            out = processor.batch_decode(generated_ids[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
            
            ans = "unknown"
            exp = out
            if "Explanation:" in out:
                try:
                    parts = out.split("Explanation:")
                    ans = parts[0].replace("Answer:", "").strip().lower()
                    exp = parts[1].strip()
                except: pass
            else:
                ans = out.split("\n")[0].strip().lower()
            
            candidates.append((ans, exp))
            
        # === æŠ•ç¥¨ ===
        answers = [c[0] for c in candidates]
        most_common_ans, count = Counter(answers).most_common(1)[0]
        
        # é€‰è§£é‡Šï¼šå¦‚æœåŸå›¾ç­”æ¡ˆå°±æ˜¯å¤šæ•°æ´¾ï¼Œç”¨åŸå›¾è§£é‡Šï¼›å¦åˆ™æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…çš„
        if candidates[0][0] == most_common_ans:
            final_exp = candidates[0][1]
        else:
            final_exp = next(c[1] for c in candidates if c[0] == most_common_ans)
            
        results.append({"id": row['id'], "answer": most_common_ans, "explanation": final_exp})
        
    pd.DataFrame(results)[['id', 'answer', 'explanation']].to_csv(OUTPUT_FILE, index=False)
    print(f"ğŸ† TTA Submission Saved: {OUTPUT_FILE}")

if __name__ == "__main__":
    run_inference()